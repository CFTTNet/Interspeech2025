{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:numexpr.utils:Note: NumExpr detected 32 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type                    | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | model          | DCCTN                   | 10.0 M | train\n",
      "1 | si_sdr_loss    | SISDRLoss               | 0      | train\n",
      "2 | stft_loss_func | MultiResolutionSTFTLoss | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "9.8 M     Trainable params\n",
      "132 K     Non-trainable params\n",
      "10.0 M    Total params\n",
      "39.810    Total estimated model params size (MB)\n",
      "386       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04b7161059b447d9bedc52094043793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.rnn import LSTM, GRU\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Auraloss\n",
    "from auraloss.time import SISDRLoss\n",
    "from auraloss.freq import MultiResolutionSTFTLoss\n",
    "\n",
    "# Sound file\n",
    "import soundfile as sf\n",
    "\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Complex modules\n",
    "from cplxmodule.nn import CplxConv2d, CplxConvTranspose2d, CplxBatchNorm2d, CplxConv1d, CplxBatchNorm1d\n",
    "from utils import *\n",
    "from complexPyTorch.complexFunctions import complex_relu\n",
    "\n",
    "# Lightning callbacks\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('root').setLevel(logging.WARNING)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "epsilon = torch.finfo(torch.float32).eps\n",
    "\n",
    "class CustomTQDMProgressBar(Callback): #custom pytorch lightening progress bar to display the real time progress to display the real time metrics such as SISDR, STFT, total loss, and LR\n",
    "    \"\"\"\n",
    "    Displays a custom TQDM progress bar showing SISDR, STFT, total loss, and LR each batch.\n",
    "    \"\"\"\n",
    "    def __init__(self): #calls constructor of the parent callback class to ensure proper initialization\n",
    "        super().__init__()\n",
    "        self.progress_bar = None #initializes the progress_bar attribute to none, this later hold the TQDM progress bar instance\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module): #initializes the tqdm progress bar at the start of the each training epoch, trainer object controlling training process, lightenig module being trained\n",
    "        total_batches = trainer.num_training_batches #retrieves total number of training batches in the current epoch from the trainer\n",
    "        # Create the progress bar for this epoch\n",
    "        self.progress_bar = tqdm(\n",
    "            total=total_batches, #total number of iterations of the progress bar equal to the total batches\n",
    "            desc=f\"Epoch {trainer.current_epoch + 1}/{trainer.max_epochs}\", #descript prefix containing current epoch and total number of epochs\n",
    "            leave=True  # keep the progress bar after the epoch is completed \n",
    "        )\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if self.progress_bar:\n",
    "            # Grab the most recent losses from the module lists (or from `outputs` if you prefer)\n",
    "            # Make sure pl_module.train_sisdr_epoch etc. actually have entries\n",
    "            sisdr_val = pl_module.train_sisdr_epoch[-1].item() if pl_module.train_sisdr_epoch else 0.0\n",
    "            stft_val  = pl_module.train_stft_epoch[-1].item()  if pl_module.train_stft_epoch  else 0.0\n",
    "            total_val = pl_module.train_total_epoch[-1].item() if pl_module.train_total_epoch else 0.0\n",
    "\n",
    "            # Current LR from the first optimizer\n",
    "            current_lr = trainer.optimizers[0].param_groups[0]['lr']\n",
    "\n",
    "            # Update TQDM postfix\n",
    "            self.progress_bar.set_postfix({\n",
    "                'SISDR': f\"{sisdr_val:.4f}\",\n",
    "                'STFT': f\"{stft_val:.4f}\",\n",
    "                'Total': f\"{total_val:.4f}\",\n",
    "                'LR': f\"{current_lr:.2e}\"\n",
    "            })\n",
    "            self.progress_bar.update(1)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.close()\n",
    "            self.progress_bar = None\n",
    "\n",
    "# -------------- Utility function to save wav -------------- #\n",
    "def save_wav(audio_data, sr, filepath):\n",
    "    try:\n",
    "        sf.write(filepath, audio_data, sr)\n",
    "        logging.info(f\"Saved WAV file: {filepath}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save WAV file {filepath}: {e}\", exc_info=True)\n",
    "\n",
    "# -------------- Dataset for 4s Clipped WAVs -------------- #\n",
    "class FourSecDataset(Dataset): #pytorch's dataset class inheritance, make it compatible to pytorch's data loading mechanisms such as data loader\n",
    "    \"\"\"\n",
    "    Loads the already preprocessed 4-second clips from disk.\n",
    "    Expects to find filenames like:\n",
    "        acc_<something>.wav\n",
    "        clean_<something>.wav\n",
    "    in the same directory.\n",
    "\n",
    "    This does NOT do filtering/resampling/padding, because\n",
    "    it expects data is already prepared exactly as 4s, 16kHz.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_4s_dir, mode='acc', sample_rate=48000): #root directory where the audios are stored, find acc and pair with clean, sampling rate = 48000 \n",
    "        super().__init__() #base class inheritance\n",
    "        # 'mode' can be 'acc' or 'clean', but typically we find pairs.\n",
    "        self.root_4s_dir = root_4s_dir #directory path and sample rate store \n",
    "        self.sample_rate = sample_rate #expected sample rate for verification\n",
    "\n",
    "        # For example, gather only 'acc_' files, but we'll pair them with 'clean_' on the fly\n",
    "        self.acc_files = sorted([\n",
    "            f for f in os.listdir(root_4s_dir) if f.startswith('acc_') and f.lower().endswith('.wav') #list all the files start with acc and ends with .wav, sorting ensure consistent ordering\n",
    "        ])\n",
    "        if len(self.acc_files) == 0: #if do not find any acc files in the directory then raise error\n",
    "            raise RuntimeError(f\"No 'acc_' wav files found in {root_4s_dir}!\")\n",
    "\n",
    "    def __len__(self): #return the length or the number of acc files in the directory, which is essential for batching and iteration\n",
    "        return len(self.acc_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        acc_filename = self.acc_files[idx]\n",
    "        # e.g. \"acc_speaker_001.wav\" -> \"clean_speaker_001.wav\"\n",
    "        clean_filename = acc_filename.replace('acc_', 'clean_')\n",
    "\n",
    "        acc_path = os.path.join(self.root_4s_dir, acc_filename)\n",
    "        clean_path = os.path.join(self.root_4s_dir, clean_filename)\n",
    "\n",
    "        # Load them\n",
    "        acc_wav, sr = torchaudio.load(acc_path)\n",
    "        clean_wav, sr2 = torchaudio.load(clean_path)\n",
    "\n",
    "        # Optional: verify sample rates if needed\n",
    "        if sr != sr2 or sr != self.sample_rate:\n",
    "            raise RuntimeError(f\"Inconsistent sample rates. {acc_path}:{sr}, {clean_path}:{sr2} \"\n",
    "                               f\"(expected {self.sample_rate})\")\n",
    "\n",
    "        # Both should be shape [1, T]\n",
    "        return {\n",
    "            'acc': acc_wav,\n",
    "            'clean': clean_wav,\n",
    "            'filename': acc_filename  # e.g. we store the ACC file name\n",
    "        }\n",
    "class CplxLinear(nn.Module): #custom nn to handle complex-valued inputs and perform linear transformations\n",
    "    def __init__(self, in_features, out_features, bias=True): #number of input and output features and whether to include a bias term in the linear transformation\n",
    "        super(CplxLinear, self).__init__()\n",
    "        self.real_linear = nn.Linear(in_features, out_features, bias=bias) #real valued linear layer which will process the real parts of the complex input\n",
    "        self.imag_linear = nn.Linear(in_features, out_features, bias=bias) #imaginary valued linear layer which will process the imaginary parts of the complex input\n",
    "    \n",
    "    def forward(self, input): #w⋅z=(a+ib)(x+iy)=(ax−by)+i(ay+bx)\n",
    "        # Assuming input is a ComplexTensor with .real and .imag attributes\n",
    "        real = self.real_linear(input.real) - self.imag_linear(input.imag) #real = Weight_real * input.real - Weight_imag * input.imag + bias_real\n",
    "        imag = self.real_linear(input.imag) + self.imag_linear(input.real) #imaginary = Weight_real * input.imag + Weight_imaginary * input.real + bias_imaginary\n",
    "        return ComplexTensor(real, imag)\n",
    "    \n",
    "    \n",
    "class ComplexEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=False,\n",
    "                 DSC=False):\n",
    "        super(ComplexEncoder, self).__init__()\n",
    "        # DSC: depthwise_separable_conv\n",
    "        if DSC:\n",
    "            self.conv = DSC_Encoder(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                    bias=bias)\n",
    "        else:\n",
    "            self.conv = CplxConv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                   bias=bias)\n",
    "        self.norm = CplxBatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return complex_relu(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "class ComplexDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1),\n",
    "                 output_padding=(0, 0), bias=False, DSC=False):\n",
    "        super(ComplexDecoder, self).__init__()\n",
    "        # DSC: depthwise_separable_conv\n",
    "        if DSC:\n",
    "            self.conv = DSC_Decoder(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                    output_padding=output_padding, bias=bias)\n",
    "        else:\n",
    "            self.conv = CplxConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                            padding=padding, output_padding=output_padding, bias=bias)\n",
    "\n",
    "        self.norm = CplxBatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return complex_relu(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "class DSC_Encoder(nn.Module):\n",
    "    # depthwise_separable_conv\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=(2, 1), padding=(1, 1), bias=False):\n",
    "        super(DSC_Encoder, self).__init__()\n",
    "        self.depthwise = CplxConv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n",
    "                                    groups=in_channels, bias=bias)\n",
    "        self.pointwise = CplxConv2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DSC_Decoder(nn.Module):\n",
    "    # depthwise_separable_conv\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=(2, 1), padding=(1, 1), output_padding=(0, 0),\n",
    "                 bias=False):\n",
    "        super(DSC_Decoder, self).__init__()\n",
    "        self.depthwise = CplxConvTranspose2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                             padding=padding, groups=in_channels, output_padding=output_padding,\n",
    "                                             bias=bias)\n",
    "        self.pointwise = CplxConvTranspose2d(in_channels, out_channels, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "# ===================== ComplexGRU =====================\n",
    "class ComplexGRU(nn.Module): #inherits from nn.module, which is foundational class for all NN modules in PyTorch\n",
    "    def __init__(self, input_size, output_size, num_layers): #ComplexGRU integrate with pytorch components,  #number of expected features in input, output \n",
    "        super(ComplexGRU, self).__init__()\n",
    "        self.rGRU = nn.Sequential(nn.GRU(input_size=input_size, hidden_size=input_size // 2, num_layers=num_layers, batch_first=True, bidirectional=True), SelectItem(0)), #input and output is provided in the following batch, sequence, feature \n",
    "        \n",
    "        self.iGRU = nn.Sequential(\n",
    "            nn.GRU(input_size=input_size, hidden_size=input_size // 2, num_layers=num_layers, batch_first=True,\n",
    "                   bidirectional=True), \n",
    "            SelectItem(0) #hidden state will be discarded, only output will selected\n",
    "        )\n",
    "        self.linear = CplxLinear(input_size, output_size) #complex linear transformation to convert GRU outputs from input_size to output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, channel, T, F], but we typically pass [B, channel, T], or something similar\n",
    "        x = x.transpose(-1, -2).contiguous()  # [B, channel, T, F] --> [B, channel, F, T | Contiguous ensures that tensor is stored in a contiguous chunk of memory for efficient computation\n",
    "        real = self.rGRU(x.real) - self.iGRU(x.imag) \n",
    "        imag = self.rGRU(x.imag) + self.iGRU(x.real)\n",
    "        out = self.linear(ComplexTensor(real, imag)).transpose(-1, -2)\n",
    "        return out\n",
    "\n",
    "# Step: 4.2 --------------------  Complex Transformer layers --------------------------\n",
    "\n",
    "\n",
    "class Transformer_single(nn.Module): #encaptulates a single transformer encoder layer and applies it to the input tensor after appropriate reshaping and permutation\n",
    "    def __init__(self, nhead=8):\n",
    "        super(Transformer_single, self).__init__()\n",
    "        self.nhead = nhead\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.randn(10, 2, 80, 256) [batch, Ch, F, T]\n",
    "        b, c, F, T = x.shape\n",
    "        STB = TransformerEncoderLayer(d_model=F, nhead=self.nhead)  # d_model = Expected feature\n",
    "        STB.to(\"cuda\")\n",
    "        x = x.permute(1, 0, 3, 2).contiguous().view(-1, b * T, F)  # [c, b*T, F]\n",
    "        x = x.to(\"cuda\")\n",
    "        x = STB(x)\n",
    "        x = x.view(b, c, F, T)  # [b, c, F, T]\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer_multi(nn.Module):\n",
    "    # d_model = x.shape[3]\n",
    "    def __init__(self, nhead, layer_num=2):\n",
    "        super(Transformer_multi, self).__init__()\n",
    "        self.layer_num = layer_num\n",
    "        self.MTB = Transformer_single(nhead=nhead)  # d_model: the number of expected features in the input\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.layer_num):\n",
    "            x = self.MTB(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ComplexTransformer(nn.Module):\n",
    "    def __init__(self, nhead, num_layer):\n",
    "        super(ComplexTransformer, self).__init__()\n",
    "        self.rTrans = Transformer_multi(nhead=nhead, layer_num=num_layer)  # d_model = x.shape[3]\n",
    "        self.iTrans = Transformer_multi(nhead=nhead, layer_num=num_layer)  # d_model = x.shape[3]\n",
    "        # self.Trans = Transformer_multi(nhead=17, layer_num=num_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # real = self.Trans(x.real)\n",
    "        # imag = self.Trans(x.imag)\n",
    "        real = self.rTrans(x.real) - self.iTrans(x.imag)\n",
    "        imag = self.rTrans(x.imag) + self.iTrans(x.real)\n",
    "        out = ComplexTensor(real, imag)  # .contiguous()\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "    Examples::\n",
    "        # >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        # >>> src = torch.rand(10, 32, 512)\n",
    "        # >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "    #This is a single encoder layer of transformer, incorporating multi-head self attention and feedforward network\n",
    "    #uses GRU rather than the Feedforward network used traditionally \n",
    "    def __init__(self, d_model, nhead, bidirectional=True, dropout=0, activation=\"relu\"):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout).to(\"cuda\") #focus on different part of the input sequence simultaneously\n",
    "        # Implementation of Feedforward model\n",
    "        # self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.gru = GRU(d_model, d_model * 2, 1, bidirectional=bidirectional) #replaces the FF network with GRU to capture the sequential dependencies more effectively\n",
    "        self.dropout = Dropout(dropout) #prevent overfitting\n",
    "        # self.linear2 = Linear(dim_feedforward, d_model)\n",
    "        if bidirectional:\n",
    "            self.linear2 = Linear(d_model * 2 * 2, d_model) #project GRU's output back to desired dimentionality \n",
    "        else:\n",
    "            self.linear2 = Linear(d_model * 2, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model) #normalizes the inputs to stabilize and accelerate training\n",
    "        self.norm2 = LayerNorm(d_model) #prevent overfitting by randomly zeroing some of the elements of the input tensors with probability dropout\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation) #ReLU or GeLU activation function, applies non linearity \n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> Tensor\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "        Args:\n",
    "            src: the sequnce to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "\n",
    "        # src = src.to(\"cuda\")\n",
    "        # print(\"Tensor src evice:\", src.device)\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, #applies multi head self attention to the input src\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2) #add results to the original src (residual connection)\n",
    "        src = self.norm1(src) #normalizes\n",
    "        # src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        self.gru.flatten_parameters() #passes the normalized tensor through the GRU\n",
    "        out, h_n = self.gru(src) #deletes the hidden state\n",
    "        del h_n\n",
    "        src2 = self.linear2(self.dropout(self.activation(out))) #applies activation, dropout, and linear transformation\n",
    "        src = src + self.dropout2(src2) #residual connection addes results to the original src\n",
    "        src = self.norm2(src) #normalizes the src\n",
    "        return src\n",
    "\n",
    "\n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "\n",
    "\n",
    "# ===================== FTB Blocks =====================\n",
    "class NodeReshape(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(NodeReshape, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, feature_in: torch.Tensor):\n",
    "        shape = feature_in.size()\n",
    "        batch = shape[0]\n",
    "        new_shape = [batch]\n",
    "        new_shape.extend(list(self.shape))\n",
    "        return feature_in.reshape(new_shape)\n",
    "\n",
    "\n",
    "class Freq_FC(nn.Module):\n",
    "    def __init__(self, F_dim, bias=False):\n",
    "        super(Freq_FC, self).__init__()\n",
    "        self.linear = CplxLinear(F_dim, F_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape [batch, channel, T, F], we'll transpose to [batch, channel, F, T]\n",
    "        out = x.transpose(-1, -2).contiguous()  # [B, C, F, T]\n",
    "        out = self.linear(out)\n",
    "        out = torch.complex(out.real, out.imag)\n",
    "        out = out.transpose(-1, -2).contiguous()  # [B, C, T, F]\n",
    "        return out\n",
    "\n",
    "\n",
    "class ComplexFTB(torch.nn.Module):\n",
    "    \"\"\"docstring for FTB\"\"\"\n",
    "\n",
    "    def __init__(self, F_dim, channels):\n",
    "        super(ComplexFTB, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.C_r = 5\n",
    "        self.F_dim = F_dim\n",
    "\n",
    "        self.Conv2D_1 = nn.Sequential(\n",
    "            CplxConv2d(in_channels=self.channels, out_channels=self.C_r, kernel_size=1, stride=1, padding=0),\n",
    "            CplxBatchNorm2d(self.C_r),\n",
    "\n",
    "        )\n",
    "        self.Conv1D_1 = nn.Sequential(\n",
    "            CplxConv1d(self.F_dim * self.C_r, self.F_dim, kernel_size=9, padding=4),\n",
    "            CplxBatchNorm1d(self.F_dim),\n",
    "        )\n",
    "        self.FC = Freq_FC(self.F_dim, bias=False)\n",
    "        self.Conv2D_2 = nn.Sequential(\n",
    "            CplxConv2d(2 * self.channels, self.channels, kernel_size=1, stride=1, padding=0),\n",
    "            CplxBatchNorm2d(self.channels),\n",
    "        )\n",
    "\n",
    "        self.att_inner_reshape = NodeReshape([self.F_dim * self.C_r, -1])\n",
    "        self.att_out_reshape = NodeReshape([1, F_dim, -1])\n",
    "\n",
    "    def cat(self, x, y, dim):\n",
    "        real = torch.cat([x.real, y.real], dim)\n",
    "        imag = torch.cat([x.imag, y.imag], dim)\n",
    "        return ComplexTensor(real, imag)\n",
    "\n",
    "    def forward(self, inputs, verbose=False):\n",
    "        # feature_n: [batch, channel_in_out, T, F]\n",
    "\n",
    "        _, _, self.F_dim, self.T_dim = inputs.shape\n",
    "        # Conv2D\n",
    "        out = complex_relu(self.Conv2D_1(inputs));\n",
    "        if verbose: print('Layer-1               : ', out.shape)  # [B,Cr,T,F]\n",
    "        # Reshape: [batch, channel_attention, F, T] -> [batch, channel_attention*F, T]\n",
    "        out = out.view(out.shape[0], out.shape[1] * out.shape[2], out.shape[3])\n",
    "        # out = self.att_inner_reshape(out);\n",
    "        if verbose: print('Layer-2               : ', out.shape)\n",
    "        # out = out.view(-1, self.T_dim, self.F_dim * self.C_r) ; print(out.shape) # [B,c_ftb_r*f,segment_length]\n",
    "        # Conv1D\n",
    "        out = complex_relu(self.Conv1D_1(out));\n",
    "        if verbose: print('Layer-3               : ', out.shape)  # [B,F, T]\n",
    "        # temp = self.att_inner_reshape(temp); print(temp.shape)\n",
    "        out = out.unsqueeze(1)\n",
    "        # out = out.view(-1, self.channels, self.F_dim, self.T_dim);\n",
    "        if verbose: print('Layer-4               : ', out.shape)  # [B,c_a,segment_length,1]\n",
    "        # Multiplication with input\n",
    "        out = out * inputs;\n",
    "        if verbose: print('Layer-5               : ', out.shape)  # [B,c_a,segment_length,1]*[B,c_a,segment_length,f]\n",
    "        # Frequency- FC\n",
    "        # out = torch.transpose(out, 2, 3)  # [batch, channel_in_out, T, F]\n",
    "        out = self.FC(out);\n",
    "        # if verbose: print('Layer-6               : ', out.shape)  # [B,c_a,segment_length,f]\n",
    "        # out = torch.transpose(out, 2, 3)  # [batch, channel_in_out, T, F]\n",
    "        # Concatenation with Input\n",
    "        out = self.cat(out, inputs, 1);\n",
    "        if verbose: print('Layer-7               : ', out.shape)  # [B,2*c_a,segment_length,f]\n",
    "        # Conv2D\n",
    "        outputs = complex_relu(self.Conv2D_2(out));\n",
    "        if verbose: print('Layer-8               : ', outputs.shape)  # [B,c_a,segment_length,f]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# -------------------------------- Depth wise Seperable Convolution --------------------------------\n",
    "class depthwise_separable_convx(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size=3, padding=1, bias=False):\n",
    "        super(depthwise_separable_convx, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, padding=padding, groups=nin, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# ===================== Skip Blocks / Connections =====================\n",
    "class SkipBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, DSC=False):\n",
    "        super(SkipBlock, self).__init__()\n",
    "        if DSC:\n",
    "            self.conv = DSC_Encoder(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                    padding=padding, bias=True)\n",
    "        else:\n",
    "            self.conv = CplxConv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                                   padding=padding, bias=True)\n",
    "        self.norm = CplxBatchNorm2d(in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return complex_relu(self.norm(self.conv(x))) + x\n",
    "\n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    SkipConnection is a concatenation of SkipBlocks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, num_convblocks, DSC=False):\n",
    "        super(SkipConnection, self).__init__()\n",
    "        self.skip_blocks = nn.ModuleList([\n",
    "            SkipBlock(in_channels, in_channels, kernel_size=3, stride=1, padding=1, DSC=DSC)\n",
    "            for _ in range(num_convblocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for block in self.skip_blocks:\n",
    "            out = block(out)\n",
    "        return out\n",
    "\n",
    "# ===================== CFTNet Model Definition =====================\n",
    "class DCCTN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    FTBComplexSkipConvNet + Transformer\n",
    "    Complex Skip convolution\n",
    "    It uses only two FTB layers; one in the first layer and one in the last layer\n",
    "    Instead of using LSTM, it uses transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, L=256, N=256, H=128, Mask=[5, 7], B=24, F_dim=129):\n",
    "        super().__init__()\n",
    "        self.name = 'DCCTN'  # 'FTBTxComplexSkipConvNet2'\n",
    "        self.f_taps = list(range(-Mask[0] // 2 + 1, Mask[0] // 2 + 1))\n",
    "        self.t_taps = list(range(-Mask[1] // 2 + 1, Mask[1] // 2 + 1))\n",
    "\n",
    "        self.stft = STFT(frame_len=L, frame_hop=H, num_fft=N)\n",
    "        self.istft = iSTFT(frame_len=L, frame_hop=H, num_fft=N)\n",
    "\n",
    "        self.enc1 = ComplexEncoder(1, 1 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.FTB1 = ComplexFTB(math.ceil(F_dim / 2), channels=1 * B)  # First FTB layer\n",
    "        self.enc2 = ComplexEncoder(1 * B, 2 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.FTB2 = ComplexFTB(math.ceil(F_dim / 4), channels=2 * B)  # First FTB layer\n",
    "        self.enc3 = ComplexEncoder(2 * B, 2 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.FTB3 = ComplexFTB(math.ceil(F_dim / 8), channels=2 * B)  # First FTB layer\n",
    "        self.enc4 = ComplexEncoder(2 * B, 3 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.FTB4 = ComplexFTB(math.ceil(F_dim / 16), channels=3 * B)  # First FTB layer\n",
    "        self.enc5 = ComplexEncoder(3 * B, 3 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.FTB5 = ComplexFTB(math.ceil(F_dim / 32), channels=3 * B)  # First FTB layer\n",
    "        self.enc6 = ComplexEncoder(3 * B, 4 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.FTB6 = ComplexFTB(math.ceil(F_dim / 64), channels=4 * B)  # First FTB layer\n",
    "        self.enc7 = ComplexEncoder(4 * B, 4 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.FTB7 = ComplexFTB(math.ceil(F_dim / 128), channels=4 * B)  # First FTB layer\n",
    "        self.enc8 = ComplexEncoder(4 * B, 8 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.TB = ComplexTransformer(nhead=1, num_layer=2)  # d_model = x.shape[3]\n",
    "        self.GRU = ComplexGRU(8 * B, 8 * B, num_layers=2)\n",
    "\n",
    "        self.skip1 = SkipConnection(8 * B, num_convblocks=4)\n",
    "        self.skip2 = SkipConnection(4 * B, num_convblocks=4)\n",
    "        self.skip3 = SkipConnection(4 * B, num_convblocks=3)\n",
    "        self.skip4 = SkipConnection(3 * B, num_convblocks=3)\n",
    "        self.skip5 = SkipConnection(3 * B, num_convblocks=2)\n",
    "        self.skip6 = SkipConnection(2 * B, num_convblocks=2)\n",
    "        self.skip7 = SkipConnection(2 * B, num_convblocks=1)\n",
    "        self.skip8 = SkipConnection(1 * B, num_convblocks=1)\n",
    "\n",
    "        self.dec1 = ComplexDecoder(16 * B, 8 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True,\n",
    "                                   output_padding=(1, 0))\n",
    "        self.dec2 = ComplexDecoder(12 * B, 8 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.dec3 = ComplexDecoder(12 * B, 4 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.dec4 = ComplexDecoder(7 * B, 3 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.dec5 = ComplexDecoder(6 * B, 3 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.dec6 = ComplexDecoder(5 * B, 2 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.dec7 = ComplexDecoder(4 * B, 2 * B, kernel_size=(3, 3), stride=(2, 1), padding=(1, 1), bias=True)\n",
    "        self.dec8 = ComplexDecoder(3 * B, Mask[0] * Mask[1], kernel_size=(3, 3), stride=(2, 1), padding=(1, 1),\n",
    "                                   bias=True)\n",
    "\n",
    "    def cat(self, x, y, dim):\n",
    "        real = torch.cat([x.real, y.real], dim)\n",
    "        imag = torch.cat([x.imag, y.imag], dim)\n",
    "        return ComplexTensor(real, imag)\n",
    "\n",
    "    def deepfiltering(self, deepfilter, cplxInput):\n",
    "        deepfilter = deepfilter.permute(0, 2, 3, 1)\n",
    "        real_tf_shift = torch.stack(\n",
    "            [torch.roll(cplxInput.real, (i, j), dims=(1, 2)) for i in self.f_taps for j in self.t_taps], 3).transpose(\n",
    "            -1, -2)\n",
    "        imag_tf_shift = torch.stack(\n",
    "            [torch.roll(cplxInput.imag, (i, j), dims=(1, 2)) for i in self.f_taps for j in self.t_taps], 3).transpose(\n",
    "            -1, -2)\n",
    "        imag_tf_shift += 1e-10\n",
    "        cplxInput_shift = ComplexTensor(real_tf_shift, imag_tf_shift)\n",
    "        est_complex = einsum('bftd,bfdt->bft', [deepfilter.conj(), cplxInput_shift])\n",
    "        return est_complex\n",
    "\n",
    "    def forward(self, audio, verbose=False):\n",
    "        \"\"\"\n",
    "        batch: tensor of shape (batch_size x channels x num_samples)\n",
    "        \"\"\"\n",
    "        if verbose: print('*' * 60)\n",
    "        if verbose: print('Input Audio Shape         : ', audio.shape)\n",
    "        if verbose: print('*' * 60)\n",
    "\n",
    "        _, _, real, imag = self.stft(audio)\n",
    "        cplxIn = ComplexTensor(real, imag)\n",
    "        if verbose: print('STFT Complex Spec         : ', cplxIn.shape)\n",
    "\n",
    "        if verbose: print('\\n' + '-' * 20)\n",
    "        if verbose: print('Encoder Network')\n",
    "        if verbose: print('-' * 20)\n",
    "\n",
    "        enc1 = self.enc1(cplxIn.unsqueeze(1))\n",
    "        if verbose: print('Encoder-1                 : ', enc1.shape)\n",
    "        FTB1 = self.FTB1(enc1)\n",
    "        if verbose: print('FTB-1               : ', FTB1.shape)\n",
    "        enc2 = self.enc2(FTB1)\n",
    "        if verbose: print('Encoder-2                 : ', enc2.shape)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        if verbose: print('Encoder-3                 : ', enc3.shape)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        if verbose: print('Encoder-4                 : ', enc4.shape)\n",
    "        enc5 = self.enc5(enc4)\n",
    "        if verbose: print('Encoder-5                 : ', enc5.shape)\n",
    "        enc6 = self.enc6(enc5)\n",
    "        if verbose: print('Encoder-6                 : ', enc6.shape)\n",
    "        enc7 = self.enc7(enc6)\n",
    "        if verbose: print('Encoder-7                 : ', enc7.shape)\n",
    "        FTB7 = self.FTB7(enc7)\n",
    "        if verbose: print('FTB-7               : ', FTB7.shape)\n",
    "        enc8 = self.enc8(FTB7)\n",
    "        if verbose: print('Encoder-8                 : ', enc8.shape)\n",
    "\n",
    "        # +++++++++++++++++++ Expanding Path  +++++++++++++++++++++ #\n",
    "\n",
    "        MLTB = self.TB(enc8)\n",
    "        if verbose: print('Transformer-1               : ', MLTB.shape)\n",
    "        if verbose: print('\\n' + '-' * 20)\n",
    "        if verbose: print('Decoder Network')\n",
    "        if verbose: print('-' * 20)\n",
    "        dec = self.dec1(self.cat(MLTB, self.skip1(enc8), 1))\n",
    "        # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        if verbose: print('Decoder-1                 : ', dec.shape)\n",
    "        dec = self.dec2(self.cat(dec, self.skip2(enc7), 1))\n",
    "        if verbose: print('Decoder-2                 : ', dec.shape)\n",
    "        dec = self.dec3(self.cat(dec, self.skip3(enc6), 1))\n",
    "        if verbose: print('Decoder-3                 : ', dec.shape)\n",
    "        dec = self.dec4(self.cat(dec, self.skip4(enc5), 1))\n",
    "        if verbose: print('Decoder-4                 : ', dec.shape)\n",
    "        dec = self.dec5(self.cat(dec, self.skip5(enc4), 1))\n",
    "        if verbose: print('Decoder-5                 : ', dec.shape)\n",
    "        dec = self.dec6(self.cat(dec, self.skip6(enc3), 1))\n",
    "        if verbose: print('Decoder-6                 : ', dec.shape)\n",
    "        dec = self.dec7(self.cat(dec, self.skip7(enc2), 1))\n",
    "        if verbose: print('Decoder-7                 : ', dec.shape)\n",
    "        dec = self.dec8(self.cat(dec, self.skip8(enc1), 1))\n",
    "        if verbose: print('Decoder-8                 : ', dec.shape)\n",
    "\n",
    "        deepfilter = ComplexTensor(dec.real, dec.imag)\n",
    "        enhanced = self.deepfiltering(deepfilter, cplxIn)\n",
    "        enh_mag, enh_phase = enhanced.abs(), enhanced.angle()\n",
    "        audio_enh = self.istft(enh_mag, enh_phase, squeeze=True)\n",
    "        if verbose: print('*' * 60)\n",
    "        if verbose: print('Output Audio Shape        : ', audio_enh.shape)\n",
    "        if verbose: print('*' * 60)\n",
    "\n",
    "        return audio_enh\n",
    "    \n",
    "class SelectItem(nn.Module):\n",
    "    \"\"\"Select item [0] from the tuple returned by e.g. GRU output\"\"\"\n",
    "    def __init__(self, idx=0):\n",
    "        super().__init__()\n",
    "        self.idx = idx\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[self.idx]\n",
    "\n",
    "# -------------- Lightning Module -------------- #\n",
    "class DeepLearningModel(pl.LightningModule):\n",
    "    def __init__(self, net, batch_size=1, save_sample_outputs=False, save_dir='enhanced_samples'):\n",
    "        super().__init__()\n",
    "        self.model = net\n",
    "        self.batch_size = batch_size\n",
    "        self.save_sample_outputs = save_sample_outputs\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        # Loss functions\n",
    "        self.si_sdr_loss = SISDRLoss()\n",
    "        self.stft_loss_func = MultiResolutionSTFTLoss(\n",
    "            fft_sizes=[256, 512, 1024],\n",
    "            hop_sizes=[128, 256, 512],\n",
    "            win_lengths=[256, 512, 1024],\n",
    "            scale=None,\n",
    "            n_bins=128,\n",
    "            sample_rate=48000,\n",
    "            perceptual_weighting=False,\n",
    "        )\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "        # For storing epoch-level losses\n",
    "        self.train_sisdr_log = []\n",
    "        self.train_stft_log = []\n",
    "        self.train_total_log = []\n",
    "        self.val_sisdr_log = []\n",
    "        self.val_stft_log = []\n",
    "        self.val_total_log = []\n",
    "        self.train_sisdr_epoch = []\n",
    "        self.train_stft_epoch = []\n",
    "        self.train_total_epoch = []\n",
    "        self.val_sisdr_epoch = []\n",
    "        self.val_stft_epoch = []\n",
    "        self.val_total_epoch = []\n",
    "\n",
    "        self.log_file = os.path.join(self.save_dir, \"loss_logs.txt\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_losses(self, pred, target):\n",
    "        \"\"\"\n",
    "        Compute SISDR and Multi-Resolution STFT losses separately + total.\n",
    "        Expect shape [B, T] or [B, 1, T].\n",
    "        Make sure we always convert to [B, 1, T] for Auraloss.\n",
    "        \"\"\"\n",
    "        # If shape is [T], add a batch dimension => [1, T]\n",
    "        if pred.dim() == 1:\n",
    "            pred = pred.unsqueeze(0)\n",
    "        if target.dim() == 1:\n",
    "            target = target.unsqueeze(0)\n",
    "\n",
    "        # If shape is [B, T], add a channel dimension => [B, 1, T]\n",
    "        if pred.dim() == 2:\n",
    "            pred = pred.unsqueeze(1)\n",
    "        if target.dim() == 2:\n",
    "         target = target.unsqueeze(1)\n",
    "        \n",
    "        target = torch.clamp(target, min=1e-8, max=1.0)\n",
    "        pred   = torch.clamp(pred,   min=-1.0, max=1.0)\n",
    "\n",
    "        # Now pred & target are [B, 1, T]. \n",
    "        sisdr = self.si_sdr_loss(target, pred)\n",
    "        freq_loss = self.stft_loss_func(pred, target) * 5.0\n",
    "        total = sisdr + freq_loss\n",
    "        return sisdr, freq_loss, total\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        acc_data = batch['acc']\n",
    "        clean_data = batch['clean']\n",
    "\n",
    "        enh_audio = self(acc_data)\n",
    "        sisdr_loss, stft_loss, total_loss = self.compute_losses(enh_audio, clean_data)\n",
    "\n",
    "        self.train_sisdr_epoch.append(sisdr_loss)\n",
    "        self.train_stft_epoch.append(stft_loss)\n",
    "        self.train_total_epoch.append(total_loss)\n",
    "\n",
    "        self.log('train_sisdr_step', sisdr_loss, on_step=True)\n",
    "        self.log('train_stft_step', stft_loss, on_step=True)\n",
    "        self.log('train_total_step', total_loss, on_step=True)\n",
    "\n",
    "        # Optionally save some examples\n",
    "        if self.save_sample_outputs and (batch_idx < 5):\n",
    "            filenames = batch['filename']\n",
    "            for i in range(acc_data.size(0)):\n",
    "                enhanced = enh_audio[i].detach().cpu().numpy().squeeze()\n",
    "                base_name = os.path.splitext(filenames[i])[0]\n",
    "                epoch_num = self.current_epoch\n",
    "                out_filename = f\"{base_name}_epoch_{epoch_num}.wav\"\n",
    "                out_filepath = os.path.join(self.save_dir, out_filename)\n",
    "                save_wav(enhanced, 48000, out_filepath)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        sisdr_mean = torch.stack(self.train_sisdr_epoch).mean()\n",
    "        stft_mean = torch.stack(self.train_stft_epoch).mean()\n",
    "        total_mean = torch.stack(self.train_total_epoch).mean()\n",
    "\n",
    "        self.train_sisdr_log.append(sisdr_mean.item())\n",
    "        self.train_stft_log.append(stft_mean.item())\n",
    "        self.train_total_log.append(total_mean.item())\n",
    "\n",
    "        self.log('train_sisdr_epoch', sisdr_mean)\n",
    "        self.log('train_stft_epoch', stft_mean)\n",
    "        self.log('train_total_epoch', total_mean)\n",
    "\n",
    "        current_lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        print(f\"Epoch {self.current_epoch} - LR: {current_lr:.6f} | \"\n",
    "              f\"Train SISDR: {sisdr_mean:.4f}, Train STFT: {stft_mean:.4f}, Train Total: {total_mean:.4f}\")\n",
    "\n",
    "        with open(self.log_file, \"a\") as f:\n",
    "            f.write(f\"[Train] Epoch {self.current_epoch} | LR: {current_lr:.6f} | \"\n",
    "                    f\"SISDR: {sisdr_mean:.4f} | STFT: {stft_mean:.4f} | Total: {total_mean:.4f}\\n\")\n",
    "\n",
    "        self.train_sisdr_epoch.clear()\n",
    "        self.train_stft_epoch.clear()\n",
    "        self.train_total_epoch.clear()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        acc_data = batch['acc']\n",
    "        clean_data = batch['clean']\n",
    "\n",
    "        enh_audio = self(acc_data)\n",
    "        sisdr_loss, stft_loss, total_loss = self.compute_losses(enh_audio, clean_data)\n",
    "\n",
    "        self.val_sisdr_epoch.append(sisdr_loss)\n",
    "        self.val_stft_epoch.append(stft_loss)\n",
    "        self.val_total_epoch.append(total_loss)\n",
    "\n",
    "        if self.save_sample_outputs and (batch_idx < 5):\n",
    "            filenames = batch['filename']\n",
    "            for i in range(acc_data.size(0)):\n",
    "                enhanced = enh_audio[i].detach().cpu().numpy().squeeze()\n",
    "                base_name = os.path.splitext(filenames[i])[0]\n",
    "                epoch_num = self.current_epoch\n",
    "                out_filename = f\"{base_name}_epoch_{epoch_num}.wav\"\n",
    "                out_filepath = os.path.join(self.save_dir, out_filename)\n",
    "                save_wav(enhanced, 48000, out_filepath)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        sisdr_mean = torch.stack(self.val_sisdr_epoch).mean()\n",
    "        stft_mean = torch.stack(self.val_stft_epoch).mean()\n",
    "        total_mean = torch.stack(self.val_total_epoch).mean()\n",
    "\n",
    "        self.val_sisdr_log.append(sisdr_mean.item())\n",
    "        self.val_stft_log.append(stft_mean.item())\n",
    "        self.val_total_log.append(total_mean.item())\n",
    "\n",
    "        self.log('val_sisdr_epoch', sisdr_mean)\n",
    "        self.log('val_stft_epoch', stft_mean)\n",
    "        self.log('val_total_epoch', total_mean)\n",
    "\n",
    "        print(f\"Epoch {self.current_epoch} - \"\n",
    "              f\"Val SISDR: {sisdr_mean:.4f}, Val STFT: {stft_mean:.4f}, Val Total: {total_mean:.4f}\")\n",
    "\n",
    "        with open(self.log_file, \"a\") as f:\n",
    "            f.write(f\"[Val]   Epoch {self.current_epoch} | \"\n",
    "                    f\"SISDR: {sisdr_mean:.4f} | STFT: {stft_mean:.4f} | Total: {total_mean:.4f}\\n\")\n",
    "\n",
    "        self.val_sisdr_epoch.clear()\n",
    "        self.val_stft_epoch.clear()\n",
    "        self.val_total_epoch.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=1e-4,\n",
    "            weight_decay=1e-5,\n",
    "            betas=(0.5, 0.999)\n",
    "        )\n",
    "        scheduler = {\n",
    "            'scheduler': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                optimizer,\n",
    "                T_0=10,\n",
    "                T_mult=1,\n",
    "                eta_min=0.0,\n",
    "                last_epoch=-1\n",
    "            ),\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1\n",
    "        }\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "class TQDMEpochProgressBar(Callback):\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        trainer.epoch_progress = tqdm(\n",
    "            total=trainer.num_training_batches,\n",
    "            desc=f\"Epoch {trainer.current_epoch + 1}/{trainer.max_epochs}\",\n",
    "            unit='batch',\n",
    "            leave=False\n",
    "        )\n",
    "\n",
    "    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n",
    "        if hasattr(trainer, 'epoch_progress'):\n",
    "            trainer.epoch_progress.update(1)\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if hasattr(trainer, 'epoch_progress'):\n",
    "            trainer.epoch_progress.close()\n",
    "            del trainer.epoch_progress\n",
    "\n",
    "class ResourceCleanupCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# --------------------- MAIN: training usage --------------------- #\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "\n",
    "    model_name = 'DCCTN'\n",
    "    batch_size = 2\n",
    "    epochs = 100\n",
    "    gpu_ids = [0]\n",
    "    loss_function = 'SISDR+MultiResFreqLoss'\n",
    "    \n",
    "    # Point these to the directories created by data_preprocessing.py\n",
    "    train_4s_dir = r\"C:\\Users\\Anomadarshi\\Desktop\\VCTK_down_upsample_2_48\\Train_4s\\4s_clips\"\n",
    "    dev_4s_dir   = r\"C:\\Users\\Anomadarshi\\Desktop\\VCTK_down_upsample_2_48\\Dev_4s\\4s_clips\"\n",
    "\n",
    "    # Check existence\n",
    "    if not os.path.isdir(train_4s_dir):\n",
    "        raise RuntimeError(f\"Train directory {train_4s_dir} does not exist!\")\n",
    "    if not os.path.isdir(dev_4s_dir):\n",
    "        raise RuntimeError(f\"Dev directory {dev_4s_dir} does not exist!\")\n",
    "\n",
    "    # 1) Create model\n",
    "    net = DCCTN()\n",
    "    model = DeepLearningModel(net, batch_size=batch_size,\n",
    "                              save_sample_outputs=True, save_dir='enhanced_samples')\n",
    "\n",
    "    # 2) Create callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_total_epoch',\n",
    "        dirpath=os.path.join(os.getcwd(), 'Saved_Models', model_name),\n",
    "        filename=model_name + '-ACCEAR-' + loss_function + '-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=1,\n",
    "        mode='min'\n",
    "    )\n",
    "    tqdm_callback = TQDMEpochProgressBar()\n",
    "    cleanup_callback = ResourceCleanupCallback()\n",
    "    callbacks = [checkpoint_callback, tqdm_callback, cleanup_callback]\n",
    "\n",
    "    # 3) Create data loaders from the 4s clipped WAVs\n",
    "    train_dataset = FourSecDataset(train_4s_dir)\n",
    "    dev_dataset   = FourSecDataset(dev_4s_dir)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                              shuffle=True, num_workers=0, pin_memory=True, drop_last=True)\n",
    "    dev_loader   = DataLoader(dev_dataset, batch_size=batch_size,\n",
    "                              shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # 4) Trainer\n",
    "    trainer_kwargs = {\n",
    "        'max_epochs': epochs,\n",
    "        'accelerator': 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "        'devices': [0],\n",
    "        'callbacks': callbacks,\n",
    "        'gradient_clip_val': 10,\n",
    "        'accumulate_grad_batches': 8,\n",
    "        'log_every_n_steps': 10,\n",
    "        'precision': 32,\n",
    "        'enable_checkpointing': True,\n",
    "        'enable_progress_bar': True,\n",
    "        'num_sanity_val_steps': 0\n",
    "    }\n",
    "    if len(gpu_ids) > 1 and torch.cuda.is_available():\n",
    "        trainer_kwargs['strategy'] = 'ddp'\n",
    "\n",
    "    trainer = pl.Trainer(**trainer_kwargs)\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=dev_loader)\n",
    "\n",
    "    # 5) Save \"last epoch\" model\n",
    "    last_model_path = os.path.join(checkpoint_callback.dirpath, \"model_last.ckpt\")\n",
    "    trainer.save_checkpoint(last_model_path)\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    # 6) Plot or visualize losses if desired\n",
    "    epochs_range = range(len(model.train_sisdr_log))\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(311)\n",
    "    plt.plot(epochs_range, model.train_sisdr_log, 'b-o', label='Train SISDR')\n",
    "    plt.plot(epochs_range, model.val_sisdr_log, 'r-o', label='Val SISDR')\n",
    "    plt.title(\"SISDR Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(312)\n",
    "    plt.plot(epochs_range, model.train_stft_log, 'b-o', label='Train STFT')\n",
    "    plt.plot(epochs_range, model.val_stft_log, 'r-o', label='Val STFT')\n",
    "    plt.title(\"MultiRes STFT Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(313)\n",
    "    plt.plot(epochs_range, model.train_total_log, 'b-o', label='Train Total')\n",
    "    plt.plot(epochs_range, model.val_total_log, 'r-o', label='Val Total')\n",
    "    plt.title(\"Total Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    loss_plot_path = os.path.join(checkpoint_callback.dirpath, \"loss_components_plot.png\")\n",
    "    plt.savefig(loss_plot_path)\n",
    "    print(f\"Saved loss plot to {loss_plot_path}\")\n",
    "\n",
    "    # 7) (Optional) Enhance dev set with best/last model and save\n",
    "\n",
    "    best_model_path = checkpoint_callback.best_model_path\n",
    "    print(f\"Best model checkpoint path: {best_model_path}\")\n",
    "\n",
    "    best_model = DeepLearningModel.load_from_checkpoint(\n",
    "        best_model_path,\n",
    "        net=DCCTN()\n",
    "    )\n",
    "    best_model.eval().cuda()\n",
    "\n",
    "    last_model = DeepLearningModel.load_from_checkpoint(\n",
    "        last_model_path,\n",
    "        net=DCCTN()\n",
    "    )\n",
    "    last_model.eval().cuda()\n",
    "\n",
    "    def enhance_and_save(pl_model, dataset, out_folder):\n",
    "        os.makedirs(out_folder, exist_ok=True)\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        pl_model.to(device)\n",
    "        pl_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(dataset)):\n",
    "                sample = dataset[i]\n",
    "                acc_data = sample['acc'].unsqueeze(0).to(device)\n",
    "                enhanced = pl_model(acc_data)\n",
    "                enhanced_np = enhanced.squeeze().cpu().numpy()\n",
    "                base_name = os.path.splitext(sample['filename'])[0]\n",
    "                save_path = os.path.join(out_folder, f\"{base_name}.wav\")\n",
    "                save_wav(enhanced_np, 48000, save_path)\n",
    "\n",
    "    dev_enh_best = os.path.join(checkpoint_callback.dirpath, \"Enhanced_Best\")\n",
    "    dev_enh_last = os.path.join(checkpoint_callback.dirpath, \"Enhanced_Last\")\n",
    "\n",
    "    print(f\"Generating enhanced audio with best model into {dev_enh_best}...\")\n",
    "    enhance_and_save(best_model, dev_dataset, dev_enh_best)\n",
    "\n",
    "    print(f\"Generating enhanced audio with last model into {dev_enh_last}...\")\n",
    "    enhance_and_save(last_model, dev_dataset, dev_enh_last)\n",
    "\n",
    "    print(\"Enhancement complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
